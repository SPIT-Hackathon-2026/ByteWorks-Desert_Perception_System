import os, sys, random, argparse, json, math
functional as TF
on, SegformerConfig
executable, "-m", "pip", "install",
on, SegformerConfig


# ═══════════════════════════════════════════════════════════════════════════
# CONFIG
# ═══════════════════════════════════════════════════════════════════════════
Training_Dataset/train/Color_Images",
    train_mask_dir   = "Offroad_Segmentation_Training_Dataset/train/Segmentation",
    val_img_dir      = "Offroad_Segmentation_Training_Dataset/val/Color_Images",
    val_mask_dir     = "Offroad_Segmentation_Training_Dataset/val/Segmentation",
    test_img_dir     = "Offroad_Segmentation_testImages/Color_Images",
    test_mask_dir    = "Offroad_Segmentation_testImages/Segmentation",  # optional
    epochs           = 100,
    batch_size       = 4,
    lr               = 6e-5,
    weight_decay     = 0.01,
    warmup_epochs    = 8,
    num_workers      = 4,
    seed             = 42,
    # Loss hyper-params
    focal_gamma      = 2.0,   # >0 enables FocalLoss; set 0 for plain CE
    label_smoothing  = 0.05,  # smooths noisy GT; 0 disables
    ce_weight        = 0.5,
    dice_weight      = 0.3,
    focal_weight     = 0.2,
    # Class weights file (auto-generated by --mode compute_weights)
    class_weights_file = "outputs/class_weights.json",
    # TTA at test time
    use_tta          = True,
    # Confidence suppression: predictions below this threshold for minority
    # classes (all except landscape/0) are replaced with class 0.
    conf_threshold   = 0.0,   # 0 disables; try 0.3 if over-predicting minorities
    # CutMix probability during training
    cutmix_prob      = 0.3,
)

# ─── Class palette ─────────────────────────────────────────────────────────
# Run `python train_segment.py --mode probe` to auto-print the real colours.
═════════════════════════════════════════════
# UTILITIES
# ═══════════════════════════════════════════════════════════════════════════
def use_amp(device: torch.device) -> bool:
    return device.type == "cuda"


def mask_rgb_to_index(mask_np: np.ndarray) -> np.ndarray:
    """RGB (H×W×3) or label (H×W) mask → H×W int64 class indices."""
    n = CFG["num_classes"]
    if mask_np.ndim == 2:
        return np.clip(mask_np.astype(np.int64), 0, n - 1)

    H, W = mask_np.shape[:2]
    out     = np.zeros((H, W), dtype=np.int64)
    matched = np.zeros((H, W), dtype=bool)

    for rgb, cls in COLOR2CLASS.items():
        m = np.all(mask_np == np.array(rgb, dtype=np.uint8), axis=-1)
        out[m & ~matched] = cls
        matched |= m

    # Nearest-colour fallback for unmatched pixels
    if not matched.all():
        pal  = np.array(list(COLOR2CLASS.keys()),   dtype=np.float32)
        vals = np.array(list(COLOR2CLASS.values()), dtype=np.int64)
        ux, uy = np.where(~matched)
        pix    = mask_np[ux, uy].astype(np.float32)
        dists  = np.linalg.norm(pix[:, None] - pal[None], axis=2)
        out[ux, uy] = vals[dists.argmin(axis=1)]
    return out


def index_to_color(idx: np.ndarray) -> np.ndarray:
    rgb = np.zeros((*idx.shape, 3), dtype=np.uint8)
    for i, c in enumerate(VIS_COLORS):
        rgb[idx == i] = c
    return rgb


def save_figure(img_np, pred_idx, gt_idx=None, out_path="pred.png", title=""):
    ncols = 3 if gt_idx is not None else 2
    fig, axes = plt.subplots(1, ncols, figsize=(6 * ncols, 5))
    fig.suptitle(title, fontsize=11)

    axes[0].imshow(img_np)
    axes[0].set_title("Input Image"); axes[0].axis("off")

    if gt_idx is not None:
        axes[1].imshow(index_to_color(gt_idx))
        axes[1].set_title("Ground Truth"); axes[1].axis("off")
        axes[2].imshow(index_to_color(pred_idx))
        axes[2].set_title("Prediction"); axes[2].axis("off")
    else:
        axes[1].imshow(index_to_color(pred_idx))
        axes[1].set_title("Prediction"); axes[1].axis("off")

    patches = [mpatches.Patch(color=np.array(c)/255, label=CLASS_NAMES[i])
               for i, c in enumerate(VIS_COLORS)]
    fig.legend(handles=patches, loc="lower center", ncol=5,
               fontsize=7, bbox_to_anchor=(0.5, -0.03))
    plt.tight_layout()
    plt.savefig(out_path, dpi=150, bbox_inches="tight")
    plt.close()


# ═══════════════════════════════════════════════════════════════════════════
# AUGMENTATION  (pure PIL / torchvision)
# ═══════════════════════════════════════════════════════════════════════════
class PairedTransform:
    """Identical spatial transforms applied to both image and mask."""

    def __init__(self, split: str, img_size: int):
        self.split = split
        self.sz    = img_size
        self.jitter = T.ColorJitter(
            brightness=0.40, contrast=0.40, saturation=0.40, hue=0.08)

    def __call__(self, img: Image.Image, mask: Image.Image):
        sz = self.sz

        # Always resize to target first
        img  = img .resize((sz, sz), Image.BILINEAR)
        mask = mask.resize((sz, sz), Image.NEAREST)

        if self.split == "train":
            # ── Random scale + crop ──────────────────────────────────────
            scale = random.uniform(0.5, 2.0)          # wider range
            new_s = int(sz * scale)
            img  = img .resize((new_s, new_s), Image.BILINEAR)
            mask = mask.resize((new_s, new_s), Image.NEAREST)
            if new_s > sz:
                x = random.randint(0, new_s - sz)
                y = random.randint(0, new_s - sz)
                img  = img .crop((x, y, x + sz, y + sz))
                mask = mask.crop((x, y, x + sz, y + sz))
            else:
                img  = img .resize((sz, sz), Image.BILINEAR)
                mask = mask.resize((sz, sz), Image.NEAREST)

            # ── Horizontal flip ──────────────────────────────────────────
            if random.random() > 0.5:
                img  = TF.hflip(img)
                mask = TF.hflip(mask)

            # ── Vertical flip ────────────────────────────────────────────
            if random.random() > 0.7:
                img  = TF.vflip(img)
                mask = TF.vflip(mask)

            # ── Rotation ±20 deg ─────────────────────────────────────────
            if random.random() > 0.5:
                angle = random.uniform(-20, 20)
                img  = TF.rotate(img,  angle,
                                 interpolation=TF.InterpolationMode.BILINEAR)
                mask = TF.rotate(mask, angle,
                                 interpolation=TF.InterpolationMode.NEAREST)

            # ── Colour jitter (image only) ───────────────────────────────
            if random.random() > 0.3:
                img = self.jitter(img)

            # ── Random grayscale ─────────────────────────────────────────
            if random.random() > 0.85:
                img = TF.rgb_to_grayscale(img, num_output_channels=3)

            # ── Gaussian blur ────────────────────────────────────────────
            if random.random() > 0.6:
                r = random.choice([1, 2, 3])
                img = img.filter(ImageFilter.GaussianBlur(radius=r))

            # ── Brightness shift ─────────────────────────────────────────
            if random.random() > 0.5:
                img = ImageEnhance.Brightness(img).enhance(
                    random.uniform(0.5, 1.5))

            # ── Sharpness ────────────────────────────────────────────────
            if random.random() > 0.7:
                img = ImageEnhance.Sharpness(img).enhance(
                    random.uniform(0.5, 2.0))

        # ── To tensor + ImageNet normalise ───────────────────────────────
        img_t  = TF.normalize(TF.to_tensor(img), IMAGENET_MEAN, IMAGENET_STD)
        mask_t = torch.from_numpy(
            np.array(mask, dtype=np.int64)
        ).clamp(0, CFG["num_classes"] - 1)
        return img_t, mask_t


def cutmix_batch(imgs: torch.Tensor, masks: torch.Tensor,
                 alpha: float = 1.0):
    """
    CutMix: paste a rectangular patch from a shuffled sample.
    Returns augmented (imgs, masks) tensors.
    """
    B, C, H, W = imgs.shape
    if B < 2:
        return imgs, masks

    lam  = np.random.beta(alpha, alpha)
    idx  = torch.randperm(B, device=imgs.device)

    cut_ratio = math.sqrt(1.0 - lam)
    cut_h = int(H * cut_ratio)
    cut_w = int(W * cut_ratio)

    cx = random.randint(0, W)
    cy = random.randint(0, H)
    x1, x2 = max(0, cx - cut_w // 2), min(W, cx + cut_w // 2)
    y1, y2 = max(0, cy - cut_h // 2), min(H, cy + cut_h // 2)

    imgs_new  = imgs.clone()
    masks_new = masks.clone()
    imgs_new [:, :, y1:y2, x1:x2] = imgs[idx, :, y1:y2, x1:x2]
    masks_new[:,    y1:y2, x1:x2] = masks[idx,    y1:y2, x1:x2]
    return imgs_new, masks_new


# ═══════════════════════════════════════════════════════════════════════════
# DATASETS
# ═══════════════════════════════════════════════════════════════════════════
def _collect(d: str):
    p = Path(d)
    imgs = []
    for ext in ("*.png", "*.jpg", "*.jpeg"):
        imgs += sorted(p.glob(ext))
    if not imgs:
        raise FileNotFoundError(f"No images found in: {d}")
    return imgs


class SegDataset(Dataset):
    def __init__(self, img_dir: str, mask_dir: str, split: str = "train"):
        self.paths    = _collect(img_dir)
        self.mask_dir = Path(mask_dir)
        self.tf       = PairedTransform(split, CFG["img_size"])
        print(f"  [{split:5s}] {len(self.paths)} images  ← {img_dir}")

    def __len__(self):
        return len(self.paths)

    def _load_mask(self, img_path: Path) -> Image.Image:
        for ext in (".png", ".jpg", ".bmp"):
            mp = self.mask_dir / (img_path.stem + ext)
            if mp.exists():
                raw = np.array(Image.open(mp).convert("RGB"))
                idx = mask_rgb_to_index(raw).astype(np.int32)
                return Image.fromarray(idx, mode="I")
        raise FileNotFoundError(
            f"No mask for '{img_path.name}' in {self.mask_dir}")

    def __getitem__(self, i):
        p    = self.paths[i]
        img  = Image.open(p).convert("RGB")
        mask = self._load_mask(p)
        img_t, mask_t = self.tf(img, mask)
        return img_t, mask_t, p.name


class TestDataset(Dataset):
    """Test images + optional GT masks."""

    def __init__(self, img_dir: str, mask_dir=None):
        self.paths    = _collect(img_dir)
        self.mask_dir = Path(mask_dir) if mask_dir else None
        self.sz       = CFG["img_size"]
        print(f"  [test ] {len(self.paths)} images  ← {img_dir}")
convert("RGB"))
r), IMAGENET_MEAN, IMAGENET_STD)
═════════════════════════════════════════════
# MODEL
# ═══════════════════════════════════════════════════════════════════════════
on.from_pretrained(
pretrained(name)
on(cfg)
    return model


# ═══════════════════════════════════════════════════════════════════════════
# LOSSES
# ═══════════════════════════════════════════════════════════════════════════
class FocalLoss(nn.Module):
    """
    Multi-class Focal Loss.
    gamma=0 reduces to standard cross-entropy.
    """
    def __init__(self, gamma: float = 2.0, weight=None,
                 label_smoothing: float = 0.0):
        super().__init__()
        self.gamma   = gamma
        self.weight  = weight          # class weights tensor or None
        self.smooth  = label_smoothing

    def forward(self, logits: torch.Tensor, targets: torch.Tensor):
        # logits: (B, C, H, W)   targets: (B, H, W)
        C = logits.shape[1]
        # Compute per-pixel log-probabilities
        log_p = F.log_softmax(logits, dim=1)             # (B, C, H, W)
        p     = log_p.exp()

        # Gather the log-prob of the true class at each pixel
        tgt_clipped = targets.clamp(0, C - 1).unsqueeze(1)  # (B,1,H,W)
        log_pt = log_p.gather(1, tgt_clipped).squeeze(1)     # (B,H,W)
        pt     = p    .gather(1, tgt_clipped).squeeze(1)     # (B,H,W)

        # Focal modulation
        focal_mod = (1.0 - pt) ** self.gamma

        # Optional label smoothing: mix with uniform distribution
        if self.smooth > 0:
            log_pt = (1 - self.smooth) * log_pt + \
                     self.smooth * log_p.mean(dim=1)

        loss = -focal_mod * log_pt                           # (B,H,W)

        # Class weighting
        if self.weight is not None:
            w   = self.weight.to(logits.device)
            wt  = w[targets.clamp(0, C - 1)]                # (B,H,W)
            loss = loss * wt

        return loss.mean()


class DiceLoss(nn.Module):
    def __init__(self, smooth: float = 1e-6):
        super().__init__()
        self.s = smooth

    def forward(self, logits: torch.Tensor, targets: torch.Tensor):
        n  = logits.shape[1]
        p  = torch.softmax(logits, dim=1)
        oh = torch.zeros_like(p).scatter_(
            1, targets.clamp(0, n - 1).unsqueeze(1), 1.0)
        inter = (p * oh).sum((0, 2, 3))
        union = (p + oh).sum((0, 2, 3))
        return 1.0 - ((2 * inter + self.s) / (union + self.s)).mean()


class ComboLoss(nn.Module):
    def __init__(self, class_weights=None):
        super().__init__()
        w = torch.tensor(class_weights, dtype=torch.float32) \
            if class_weights is not None else None

        self.ce = nn.CrossEntropyLoss(
            weight=w, ignore_index=255,
            label_smoothing=CFG["label_smoothing"])
        self.focal = FocalLoss(
            gamma=CFG["focal_gamma"], weight=w,
            label_smoothing=CFG["label_smoothing"])
        self.dice  = DiceLoss()

        self.cw = CFG["ce_weight"]
        self.fw = CFG["focal_weight"]
        self.dw = CFG["dice_weight"]

    def forward(self, logits, targets):
        ce_l    = self.ce   (logits, targets)
        focal_l = self.focal(logits, targets)
        dice_l  = self.dice (logits, targets)
        return self.cw * ce_l + self.fw * focal_l + self.dw * dice_l


# ═══════════════════════════════════════════════════════════════════════════
# METRICS
# ═══════════════════════════════════════════════════════════════════════════
class SegMetrics:
    def __init__(self, n: int):
        self.n = n
        self.reset()

    def reset(self):
        self.conf = np.zeros((self.n, self.n), dtype=np.int64)

    def update(self, pred: np.ndarray, gt: np.ndarray):
        mask = (gt >= 0) & (gt < self.n)
        flat = self.n * gt[mask].astype(np.int64) + pred[mask].astype(np.int64)
        self.conf += (np.bincount(flat, minlength=self.n * self.n)
                      .reshape(self.n, self.n))

    def iou_per_class(self):
        tp    = np.diag(self.conf)
        fp    = self.conf.sum(0) - tp
        fn    = self.conf.sum(1) - tp
        denom = tp + fp + fn
        return np.where(denom > 0, tp / denom, np.nan)

    def miou(self):
        return float(np.nanmean(self.iou_per_class()))

    def pixel_acc(self):
        return float(np.diag(self.conf).sum() / (self.conf.sum() + 1e-8))


# ═══════════════════════════════════════════════════════════════════════════
# CLASS WEIGHT COMPUTATION
# ═══════════════════════════════════════════════════════════════════════════
def compute_class_weights():
    """
    Scan all training masks, count pixel frequencies, and save inverse-
    frequency class weights to CFG["class_weights_file"].
    """
    mask_dir = Path(CFG["data_root"]) / CFG["train_mask_dir"]
    n        = CFG["num_classes"]
    counts   = np.zeros(n, dtype=np.float64)

    paths = []
    for ext in ("*.png", "*.jpg", "*.bmp"):
        paths += sorted(mask_dir.glob(ext))

    print(f"\n  Scanning {len(paths)} training masks for class frequencies …")
    for mp in tqdm(paths, desc="  counting"):
        raw = np.array(Image.open(mp).convert("RGB"))
        idx = mask_rgb_to_index(raw)
        for c in range(n):
            counts[c] += (idx == c).sum()

    total   = counts.sum()
    freq    = counts / (total + 1e-8)
    # Inverse-frequency weighting, normalised so mean weight = 1
    weights = 1.0 / (freq + 1e-6)
    weights = weights / weights.mean()
    weights = np.clip(weights, 0.05, 50.0)   # cap extreme values

    out = Path(CFG["class_weights_file"])
    out.parent.mkdir(parents=True, exist_ok=True)
    with open(out, "w") as f:
        json.dump({CLASS_NAMES[i]: float(weights[i]) for i in range(n)}, f,
                  indent=2)

    print(f"\n  Class pixel frequencies and weights:")
    for i in range(n):
        bar = "#" * int(min(freq[i] * 500, 40))
        print(f"    {CLASS_NAMES[i]:16s}  freq={freq[i]*100:6.2f}%  "
              f"w={weights[i]:6.2f}  {bar}")
    print(f"\n  Saved → {out}")
    return weights.tolist()


def load_class_weights():
    fp = Path(CFG["class_weights_file"])
    if not fp.exists():
        print(f"  [INFO] No class weights file at {fp}. "
              f"Run --mode compute_weights or training without weighting.")
        return None
    with open(fp) as f:
        d = json.load(f)
    weights = [d[name] for name in CLASS_NAMES]
    print(f"  Loaded class weights: "
          + ", ".join(f"{w:.2f}" for w in weights))
    return weights


# ═══════════════════════════════════════════════════════════════════════════
# INFERENCE HELPERS
# ═══════════════════════════════════════════════════════════════════════════
@torch.no_grad()
def predict_tta(model, img_t: torch.Tensor, sz: int,
                device: torch.device) -> torch.Tensor:
    """
    Multi-scale + flip TTA.
    Returns averaged softmax logits of shape (C, H, W) in float32.
    """
    scales = [1.0, 0.75, 1.25] if CFG["use_tta"] else [1.0]
    flips  = [False, True]      if CFG["use_tta"] else [False]

    accum = None
    cnt   = 0

    for scale in scales:
        for flip in flips:
            x = img_t.clone()
            if flip:
                x = torch.flip(x, dims=[-1])
            if scale != 1.0:
                new_sz = int(sz * scale)
                x = F.interpolate(x, size=(new_sz, new_sz),
                                  mode="bilinear", align_corners=False)

            with torch.amp.autocast("cuda", enabled=use_amp(device)):
                out    = model(pixel_values=x.to(device))
                logits = F.interpolate(out.logits, size=(sz, sz),
                                       mode="bilinear", align_corners=False)

            probs = torch.softmax(logits, dim=1)  # (B, C, sz, sz)

            if flip:
                probs = torch.flip(probs, dims=[-1])

            if accum is None:
                accum = probs
            else:
                accum = accum + probs
            cnt += 1

    return accum / cnt   # (B, C, sz, sz)


def apply_confidence_suppression(prob: np.ndarray,
                                  threshold: float) -> np.ndarray:
    """
    For each minority class (index > 0), if the predicted probability is
    below `threshold`, fall back to class 0 (landscape).
    prob: (C, H, W)  float32
    """
    if threshold <= 0:
        return prob.argmax(0).astype(np.int64)
    pred = prob.argmax(0).astype(np.int64)
    max_p = prob.max(0)
    # Suppress uncertain minority predictions
    suppress = (pred > 0) & (max_p < threshold)
    pred[suppress] = 0
    return pred


# ═══════════════════════════════════════════════════════════════════════════
# TRAINING LOOPS
# ═══════════════════════════════════════════════════════════════════════════
def train_one_epoch(model, loader, optimizer, criterion, scaler, scheduler,
                    device):
    model.train()
    total = 0.0
    for imgs, masks, _ in tqdm(loader, desc="  train", leave=False):
        imgs, masks = imgs.to(device), masks.to(device)

        # CutMix augmentation
        if random.random() < CFG["cutmix_prob"]:
            imgs, masks = cutmix_batch(imgs, masks)

        optimizer.zero_grad()
        with torch.amp.autocast("cuda", enabled=use_amp(device)):
            out    = model(pixel_values=imgs)
            logits = F.interpolate(out.logits, size=masks.shape[-2:],
                                   mode="bilinear", align_corners=False)
            loss   = criterion(logits, masks)

        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        scaler.step(optimizer)
        scaler.update()
        scheduler.step()    # ← step after every batch (OneCycleLR)
        total += loss.item()

    return total / max(len(loader), 1)


@torch.no_grad()
def eval_one_epoch(model, loader, criterion, device):
    model.eval()
    metrics = SegMetrics(CFG["num_classes"])
    total   = 0.0

    for imgs, masks, _ in tqdm(loader, desc="  val  ", leave=False):
        imgs, masks = imgs.to(device), masks.to(device)
        with torch.amp.autocast("cuda", enabled=use_amp(device)):
            out    = model(pixel_values=imgs)
            logits = F.interpolate(out.logits, size=masks.shape[-2:],
                                   mode="bilinear", align_corners=False)
            loss   = criterion(logits, masks)
        total += loss.item()

        pred = logits.argmax(1).cpu().numpy()
        gt   = masks.cpu().numpy()
        for p, g in zip(pred, gt):
            metrics.update(p, g)

    return total / max(len(loader), 1), metrics.miou(), metrics.pixel_acc()


# ═══════════════════════════════════════════════════════════════════════════
# PROBE  (auto-detect mask palette)
# ═══════════════════════════════════════════════════════════════════════════
def probe_palette():
    mask_dir  = Path(CFG["data_root"]) / CFG["train_mask_dir"]
    all_colors: set = set()
    paths = sorted(mask_dir.glob("*.png"))[:60]
    if not paths:
        paths = sorted(mask_dir.glob("*.jpg"))[:60]

    print(f"  Scanning {len(paths)} masks in {mask_dir} …")
    for mp in paths:
        m = np.array(Image.open(mp).convert("RGB")).reshape(-1, 3)
        for row in m:
            all_colors.add(tuple(row.tolist()))

    print(f"\n  {len(all_colors)} unique colours found.")
    print("  Update COLOR2CLASS at the top of this script:\n")
    for c in sorted(all_colors):
        print(f"    {c}: ???,  # <- assign class index")


# ═══════════════════════════════════════════════════════════════════════════
# MAIN TRAIN
# ═══════════════════════════════════════════════════════════════════════════
def train():
    set_seed(CFG["seed"])
    device  = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    out_dir = Path(CFG["output_dir"])
    out_dir.mkdir(exist_ok=True)

    print(f"\n{'='*60}")
    print(f"  Offroad Segmentation | mode=train | device={device}")
    print(f"{'='*60}")

    train_ds = SegDataset(
        os.path.join(CFG["data_root"], CFG["train_img_dir"]),
        os.path.join(CFG["data_root"], CFG["train_mask_dir"]), "train")
    val_ds   = SegDataset(
        os.path.join(CFG["data_root"], CFG["val_img_dir"]),
        os.path.join(CFG["data_root"], CFG["val_mask_dir"]), "val")

    nw = min(CFG["num_workers"], os.cpu_count() or 4)
    train_ldr = DataLoader(train_ds, CFG["batch_size"], shuffle=True,
                           num_workers=nw, pin_memory=True, drop_last=True)
    val_ldr   = DataLoader(val_ds,   CFG["batch_size"], shuffle=False,
                           num_workers=nw, pin_memory=True)

    model     = build_model(pretrained=True).to(device)
    cw        = load_class_weights()
    criterion = ComboLoss(class_weights=cw)

    # ── Differential LR: encoder 10× lower than decoder ──────────────────
    enc_params = [p for n, p in model.named_parameters()
                  if "decode_head" not in n and p.requires_grad]
    dec_params = [p for n, p in model.named_parameters()
                  if "decode_head"     in n and p.requires_grad]

    base_lr = CFG["lr"]
    enc_lr  = base_lr * 0.1
    dec_lr  = base_lr

    optimizer = torch.optim.AdamW(
        [{"params": enc_params, "lr": enc_lr},
         {"params": dec_params, "lr": dec_lr}],
        weight_decay=CFG["weight_decay"])

    total_steps  = CFG["epochs"] * len(train_ldr)
    warmup_steps = CFG["warmup_epochs"] * len(train_ldr)

    # ── FIX: OneCycleLR max_lr must have one entry per param group ────────
    scheduler = torch.optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr      = [enc_lr, dec_lr],   # one per group
        total_steps = total_steps,
        pct_start   = warmup_steps / total_steps,
        anneal_strategy = "cos",
    )

    scaler = torch.amp.GradScaler("cuda", enabled=use_amp(device))

    best_miou = 0.0
    best_acc  = 0.0
    hist = {"tl": [], "vl": [], "vm": [], "va": []}

    print(f"\n  Epochs={CFG['epochs']}  BS={CFG['batch_size']}  "
          f"ImgSize={CFG['img_size']}  Model={CFG['model_name']}")
    print(f"  FocalGamma={CFG['focal_gamma']}  "
          f"LabelSmoothing={CFG['label_smoothing']}  "
          f"CutMixProb={CFG['cutmix_prob']}\n")

    for ep in range(1, CFG["epochs"] + 1):
        tl = train_one_epoch(model, train_ldr, optimizer, criterion,
                              scaler, scheduler, device)
        vl, vm, va = eval_one_epoch(model, val_ldr, criterion, device)

        hist["tl"].append(tl); hist["vl"].append(vl)
        hist["vm"].append(vm); hist["va"].append(va)

        flag = ""
        if vm > best_miou:
            best_miou = vm
            torch.save({"epoch": ep, "model": model.state_dict(),
                        "miou": best_miou, "acc": va},
                       out_dir / CFG["checkpoint"])
            flag = "  ✓ BEST mIoU"
        if va > best_acc:
            best_acc = va
            torch.save({"epoch": ep, "model": model.state_dict(),
                        "miou": vm, "acc": best_acc},
                       out_dir / "best_acc_model.pth")
            flag += "  ✓ BEST acc"

        print(f"  Ep {ep:03d}/{CFG['epochs']}  "
              f"train={tl:.4f}  val={vl:.4f}  "
              f"mIoU={vm*100:.2f}%  acc={va*100:.2f}%  "
              f"lr={optimizer.param_groups[-1]['lr']:.2e}{flag}")

    # ── Training curves ───────────────────────────────────────────────────
    fig, (a1, a2, a3) = plt.subplots(1, 3, figsize=(16, 4))
    a1.plot(hist["tl"], label="train"); a1.plot(hist["vl"], label="val")
    a1.set_title("Loss"); a1.legend(); a1.set_xlabel("Epoch")
    a2.plot(hist["vm"], color="steelblue")
    a2.set_title("Val mIoU"); a2.set_xlabel("Epoch")
    a3.plot(hist["va"], color="orange")
    a3.set_title("Val Pixel Accuracy"); a3.set_xlabel("Epoch")
    plt.tight_layout()
    plt.savefig(out_dir / "training_curves.png", dpi=150)
    print(f"\n  Curves → {out_dir}/training_curves.png")
    print(f"  Best mIoU: {best_miou*100:.2f}%   Best Acc: {best_acc*100:.2f}%")


# ═══════════════════════════════════════════════════════════════════════════
# MAIN TEST
# ═══════════════════════════════════════════════════════════════════════════
@torch.no_grad()
def test():
    device  = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    out_dir = Path(CFG["output_dir"]) / "test_predictions"
    out_dir.mkdir(parents=True, exist_ok=True)

    ckpt_path = Path(CFG["output_dir"]) / CFG["checkpoint"]
    if not ckpt_path.exists():
        raise FileNotFoundError(
            f"Checkpoint not found: {ckpt_path}\nRun --mode train first.")

    model = build_model(pretrained=False).to(device)
    ckpt  = torch.load(str(ckpt_path), map_location=device, weights_only=False)
    model.load_state_dict(ckpt["model"] if "model" in ckpt else ckpt)
    model.eval()
    print(f"\n  Loaded: {ckpt_path}  "
          f"(ep={ckpt.get('epoch','?')}  "
          f"mIoU={ckpt.get('miou',0)*100:.2f}%)")

    # Optional test masks
    test_mask_dir = None
    candidate = Path(CFG["data_root"]) / CFG["test_mask_dir"]
    if candidate.exists():
        test_mask_dir = str(candidate)
        print("  Found test masks → computing metrics")

    ds  = TestDataset(
        os.path.join(CFG["data_root"], CFG["test_img_dir"]),
        test_mask_dir)
    ldr = DataLoader(ds, batch_size=1, shuffle=False, num_workers=2,
                     collate_fn=test_collate)

    metrics = SegMetrics(CFG["num_classes"]) if test_mask_dir else None
    sz      = CFG["img_size"]

    if CFG["use_tta"]:
        print("  TTA enabled (multi-scale + hflip)")

    print(f"\n  Inferring {len(ds)} images …\n")
    for img_t, fnames, ws, hs, gts in tqdm(ldr, desc="  test"):
        fname = fnames[0]
        W, H  = ws[0], hs[0]
        gt_np = gts[0]  # np.ndarray or None

        # TTA inference → averaged probabilities
        prob = predict_tta(model, img_t, sz, device)  # (1, C, sz, sz)
        prob_np = prob[0].float().cpu().numpy()       # (C, sz, sz)

        # Confidence-based suppression + argmax
        pred_small = apply_confidence_suppression(
            prob_np, CFG["conf_threshold"]).astype(np.uint8)

        # Resize prediction to original image dimensions
        pred_idx = np.array(
            Image.fromarray(pred_small).resize((W, H), Image.NEAREST),
            dtype=np.int64)

        # Load original image for visualisation
        orig_img = np.array(
            Image.open(
                Path(CFG["data_root"]) / CFG["test_img_dir"] / fname
            ).convert("RGB"))

        stem   = Path(fname).stem
        outpng = out_dir / f"sample_{stem}.png"
        save_figure(orig_img, pred_idx, gt_idx=gt_np,
                    out_path=str(outpng), title=f"Sample: {fname}")

        if metrics is not None and gt_np is not None:
            metrics.update(pred_idx, gt_np)

    # ── Print / save metrics ──────────────────────────────────────────────
    if metrics:
        iou_cls = metrics.iou_per_class()
        print(f"\n  mIoU      : {metrics.miou()*100:.2f}%")
        print(f"  Pixel Acc : {metrics.pixel_acc()*100:.2f}%")
        print("\n  Per-class IoU:")
        for nm, iou in zip(CLASS_NAMES, iou_cls):
            bar = "#" * int(iou * 30) if not np.isnan(iou) else ""
            print(f"    {nm:16s}  {iou*100:5.1f}%  {bar}")

        report = {
            "miou":       metrics.miou(),
            "pixel_acc":  metrics.pixel_acc(),
            "per_class":  {nm: float(v)
                           for nm, v in zip(CLASS_NAMES, iou_cls)},
        }
        out_json = Path(CFG["output_dir"]) / "test_metrics.json"
        with open(out_json, "w") as f:
            json.dump(report, f, indent=2)
        print(f"\n  Metrics     → {out_json}")

    print(f"  Predictions → {out_dir}")
    print(f"  Done ({len(ds)} images)")


# ═══════════════════════════════════════════════════════════════════════════
# ENTRY POINT
# ═══════════════════════════════════════════════════════════════════════════
if __name__ == "__main__":
    ap = argparse.ArgumentParser(description="Offroad Semantic Segmentation v2")
    ap.add_argument("--mode", default="train",
                    choices=["train", "test", "both", "probe",
                             "compute_weights"])
    ap.add_argument("--checkpoint",    default=CFG["checkpoint"])
    ap.add_argument("--epochs",        type=int,   default=CFG["epochs"])
    ap.add_argument("--batch_size",    type=int,   default=CFG["batch_size"])
    ap.add_argument("--lr",            type=float, default=CFG["lr"])
    ap.add_argument("--img_size",      type=int,   default=CFG["img_size"])
    ap.add_argument("--encoder",       default=CFG["model_name"],
                    choices=["nvidia/mit-b0", "nvidia/mit-b1", "nvidia/mit-b2",
                             "nvidia/mit-b3", "nvidia/mit-b4", "nvidia/mit-b5"])
    ap.add_argument("--num_classes",   type=int,   default=CFG["num_classes"])
    ap.add_argument("--data_root",     default=CFG["data_root"])
    ap.add_argument("--output_dir",    default=CFG["output_dir"])
    ap.add_argument("--num_workers",   type=int,   default=CFG["num_workers"])
    ap.add_argument("--focal_gamma",   type=float, default=CFG["focal_gamma"])
    ap.add_argument("--label_smooth",  type=float, default=CFG["label_smoothing"])
    ap.add_argument("--conf_thresh",   type=float, default=CFG["conf_threshold"])
    ap.add_argument("--cutmix_prob",   type=float, default=CFG["cutmix_prob"])
    ap.add_argument("--tta",           action="store_true",
                    help="Enable TTA at test time (overrides CFG default)")
    ap.add_argument("--no_tta",        action="store_true",
                    help="Disable TTA at test time")
    args = ap.parse_args()

    # Apply CLI args to CFG
    CFG["epochs"]          = args.epochs
    CFG["batch_size"]      = args.batch_size
    CFG["lr"]              = args.lr
    CFG["img_size"]        = args.img_size
    CFG["model_name"]      = args.encoder
    CFG["num_classes"]     = args.num_classes
    CFG["data_root"]       = args.data_root
    CFG["output_dir"]      = args.output_dir
    CFG["checkpoint"]      = args.checkpoint
    CFG["num_workers"]     = args.num_workers
    CFG["focal_gamma"]     = args.focal_gamma
    CFG["label_smoothing"] = args.label_smooth
    CFG["conf_threshold"]  = args.conf_thresh
    CFG["cutmix_prob"]     = args.cutmix_prob
    if args.tta:    CFG["use_tta"] = True
    if args.no_tta: CFG["use_tta"] = False

    if   args.mode == "probe":           probe_palette()
    elif args.mode == "compute_weights": compute_class_weights()
    elif args.mode == "train":           train()
    elif args.mode == "test":            test()
    elif args.mode == "both":
        train()
        test()
