# ğŸœï¸ Desert Perception System â€” ByteWorks | SPIT Hackathon 2026

> **End-to-end autonomous perception for desert/off-road terrain** â€” Real-time semantic segmentation using U-MixFormer, hardware sensor fusion, and a full-stack cloud-deployed interface.

[![Frontend](https://img.shields.io/badge/Frontend-Vercel-000?style=for-the-badge&logo=vercel)](https://semantic-segmentation-raj.vercel.app)
[![Backend](https://img.shields.io/badge/Backend-Render-46E3B7?style=for-the-badge&logo=render)](https://semantic-segmentation-api.onrender.com)
[![Python](https://img.shields.io/badge/Python-3.11-blue?style=for-the-badge&logo=python)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.10-EE4C2C?style=for-the-badge&logo=pytorch)](https://pytorch.org)
[![Next.js](https://img.shields.io/badge/Next.js-14-black?style=for-the-badge&logo=next.js)](https://nextjs.org)
[![License](https://img.shields.io/badge/License-Hackathon-orange?style=for-the-badge)](./LICENSE)

---

## ğŸ“‘ Table of Contents

- [Overview](#-overview)
- [Architecture](#-system-architecture)
- [Features](#-key-features)
- [Terrain Classes](#-terrain-classes)
- [Hardware Integration](#-hardware-integration)
- [Model Performance](#-model-performance)
- [Robustness Testing](#-robustness-testing)
- [API Reference](#-api-reference)
- [Frontend](#-frontend)
- [Project Structure](#-project-structure)
- [Setup & Installation](#-setup--installation)
- [Deployment](#-deployment)
- [Tech Stack](#-tech-stack)
- [Contributors](#-contributors)

---

## ğŸ¯ Overview

The **Desert Perception System** is a multi-modal autonomous perception pipeline designed for Unmanned Ground Vehicles (UGVs) operating in sandy, arid, and off-road environments. The system fuses visual deep learning (semantic segmentation) with embedded hardware sensors (IR and Ultrasonic) to produce real-time terrain awareness and obstacle risk scores.

**Why this matters:** Conventional object detection models trained on urban datasets fail catastrophically in desert environments â€” sand dunes look like roads, rock formations occlude obstacles, and harsh lighting conditions destroy color cues. This system addresses those challenges directly with a domain-specific model, multi-spectral imaging, and a purpose-built sensor fusion layer.

### Live Demo

| Component | URL |
|---|---|
| ğŸŒ Web Frontend | [semantic-segmentation-raj.vercel.app](https://semantic-segmentation-raj.vercel.app) |
| âš™ï¸ REST API | [semantic-segmentation-api.onrender.com](https://semantic-segmentation-api.onrender.com) |
| ğŸ“– API Docs | [semantic-segmentation-api.onrender.com/docs](https://semantic-segmentation-api.onrender.com/docs) |

---

## ğŸ—ï¸ System Architecture

The system is composed of four integrated layers:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SENSOR LAYER                        â”‚
â”‚   RGB Camera  â”‚  IR Sensor  â”‚  Ultrasonic  â”‚  UV Cam   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚               â”‚              â”‚
        â–¼               â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VISION PATH  â”‚ â”‚     HARDWARE PATH        â”‚
â”‚ Preprocessing â”‚ â”‚  IR/Ultrasonic Ensemble  â”‚
â”‚   384Ã—384     â”‚ â”‚  Obstacle Risk Score     â”‚
â”‚   Normalize   â”‚ â”‚  Proximity Alerts        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                    â”‚
        â–¼                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   U-MixFormer     â”‚        â”‚
â”‚  ConvNeXt Backboneâ”‚        â”‚
â”‚  Mix-Attention    â”‚        â”‚
â”‚  Decoder (4.1M)   â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
        â”‚                    â”‚
        â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FUSION & RISK LAYER          â”‚
â”‚  7-Class Segmentation Mask           â”‚
â”‚  Obstacle Density Score              â”‚
â”‚  Terrain Complexity Index            â”‚
â”‚  Visibility Score                    â”‚
â”‚  Overall Risk Level (LOW/MED/HIGH)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           DEPLOYMENT LAYER           â”‚
â”‚   FastAPI Backend  â†â†’  Next.js SPA  â”‚
â”‚   Render (GPU)         Vercel (CDN) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Architecture: U-MixFormer

```
Input (384Ã—384Ã—3)
     â”‚
     â–¼
ConvNeXt-Tiny Backbone
     â”œâ”€â”€ Stage 1 â†’ 96 channels  (96Ã—96)
     â”œâ”€â”€ Stage 2 â†’ 192 channels (48Ã—48)
     â”œâ”€â”€ Stage 3 â†’ 384 channels (24Ã—24)
     â””â”€â”€ Stage 4 â†’ 768 channels (12Ã—12)
                â”‚
                â–¼
     U-MixFormer Decoder
         â”œâ”€â”€ Multi-scale Feature Fusion
         â”œâ”€â”€ Mix-Attention Blocks (local + global)
         â””â”€â”€ Progressive Upsampling Refinement
                â”‚
                â–¼
     Output (384Ã—384Ã—7)  â†’  7-Class Softmax Logits
```

---

## âœ¨ Key Features

- **U-MixFormer Segmentation** â€” 4.1M parameter decoder head on ConvNeXt-Tiny backbone (~32M total); ~45ms inference on RTX 3090
- **7-Class Terrain Segmentation** â€” Pixel-level classification of desert terrain into actionable categories
- **IR/Ultrasonic Sensor Fusion** â€” Hardware ensemble for proximity-based obstacle detection, independent of camera visibility
- **UV & IR Script Processing** â€” Multi-spectral analysis scripts for enhanced desert scene understanding under harsh lighting
- **Offroad-specific Training Pipeline** â€” Domain-adapted training with the Offroad Segmentation dataset + data augmentation
- **Weather Degradation Robustness** â€” Validated against synthetic FOG (intensity 0.70) and MIST (intensity 0.62) conditions
- **Real-time Risk Assessment** â€” Composite risk score (obstacle density + terrain complexity + visibility) â†’ LOW / MEDIUM / HIGH
- **3D Pipeline Visualization** â€” Interactive Three.js architecture diagram with particle flow animation (`segheads.mp4`)
- **LIME Explainability** â€” Model transparency panel showing per-region feature attribution
- **Full Cloud Deployment** â€” Vercel (frontend) + Render (GPU backend) with auto-scaling and CI/CD via GitHub push

---

## ğŸ—ºï¸ Terrain Classes

| ID | Class | Color | Description |
|---|---|---|---|
| 0 | **Sky** | `#87CEFA` | Open sky above horizon |
| 1 | **Driveable** | `#90EE90` | Safe traversable sand / path |
| 2 | **Rock** | `#808080` | Solid rock formations |
| 3 | **Obstacle** | `#FF4444` | Dynamic or unknown obstacle |
| 4 | **Grass** | `#228B22` | Sparse desert vegetation |
| 5 | **Sand** | `#F4A460` | Loose sand â€” caution zone |
| 6 | **Rough** | `#8B4513` | Uneven, difficult terrain |

---

## ğŸ”§ Hardware Integration

The `Hardware Code/` directory and `IR_UV_Scripts/` contain embedded firmware and processing scripts for the physical UGV sensor suite.

**Sensor Stack:**
- **Ultrasonic Sensor** â€” Distance-based obstacle detection, proximity alerts, range: 2cmâ€“400cm
- **IR Sensor** â€” Passive infrared obstacle presence, works in complete darkness and dust
- **UV Camera** â€” Multi-spectral capture for improved sand/rock discrimination
- **IR Camera** â€” Thermal imaging for obstacle detection in fog and dust storms

**IR/Ultrasonic Ensemble Models** (`IR_Ultrasonic Models/`) combine both sensor outputs with a lightweight fusion model to produce a hardware-level risk score that is fused with the vision pipeline's output in the final risk assessment layer.

**Image Processing Algorithms** (`Image Processing Algs/`) include classical CV preprocessing for desert-specific challenges: sand glare normalization, horizon detection, and dust haze removal prior to DL inference.

---

## ğŸ“Š Model Performance

### Inference Speed

| Device | Resolution | FPS | Latency |
|---|---|---|---|
| A100 GPU | 384Ã—384 | 25+ | ~40ms |
| RTX 3090 | 384Ã—384 | ~22 | ~45ms |
| CPU (i9) | 384Ã—384 | ~2 | ~500ms |

### Model Size

| Component | Parameters | Weight Size |
|---|---|---|
| ConvNeXt-Tiny Backbone | 28M | 105MB |
| U-MixFormer Decoder Head | 4.1M | 15.6MB |
| **Total** | **~32M** | **~120MB** |

Checkpoint: `umixformer_pipeline/checkpoints/umixformer_best.pth`

---

## ğŸŒ«ï¸ Robustness Testing

The system was validated under two synthetic weather degradation conditions applied to 50 real test images each.

| Variant | Intensity | Visual Effect | Test Images |
|---|---|---|---|
| **FOG** | 0.70 | Dense grey-white uniform veil | 50 |
| **MIST** | 0.62 | Blue-tinted soft haze (Rayleigh scattering) | 50 |

**Results Summary:**

```
FOG Degradation:
  Avg Inference:     45.2ms  |  Throughput: ~22 img/sec
  Class Distribution: Sky 8.3% | Driveable 42.1% | Obstacle 28.4% | Rock 10.2%

MIST Degradation:
  Avg Inference:     44.8ms  |  Throughput: ~22 img/sec
  Class Distribution: Sky 12.1% | Driveable 45.3% | Obstacle 25.2% | Rock 9.4%

Model Stability: âœ… Consistent across all degradation variants
```

**Output artifacts** (in `dataset/results_better/`):

```
results_better/
â”œâ”€â”€ robustness_metrics.json
â”œâ”€â”€ robustness_metrics.txt
â”œâ”€â”€ predictions_fog/
â”‚   â”œâ”€â”€ input_images/     # Degraded FOG inputs
â”‚   â”œâ”€â”€ masks/            # Raw segmentation masks
â”‚   â”œâ”€â”€ masks_color/      # Color-coded class maps
â”‚   â”œâ”€â”€ overlays/         # Input + mask blended
â”‚   â””â”€â”€ comparisons/      # [Original | GT | Pred | Overlay] side-by-side
â””â”€â”€ predictions_mist/
    â””â”€â”€ (same structure)
```

Run tests yourself:

```bash
uv run python test_robustness.py
```

---

## ğŸ“¡ API Reference

Base URL: `https://semantic-segmentation-api.onrender.com`

### `GET /api/health`
Health check. Returns model status and compute device.

```json
{ "status": "ok", "model": "U-MixFormer", "device": "cuda" }
```

### `POST /api/segment`
Upload an image for segmentation. `multipart/form-data` with field `file` (PNG or JPEG).

**Response:**

```json
{
  "original_b64": "<base64>",
  "mask_b64": "<base64>",
  "overlay_b64": "<base64>",
  "defog_b64": "<base64>",
  "class_distribution": [
    { "id": 1, "name": "Driveable", "percentage": 42.1, "color": "rgb(144, 238, 144)" }
  ],
  "inference_ms": 45.3,
  "risk_assessment": {
    "risk_score": 0.4521,
    "risk_level": "MEDIUM",
    "obstacle_density": 0.5234,
    "uncertainty": 0.3891,
    "terrain_complexity": 0.2145,
    "visibility": 0.7823
  }
}
```

### `GET /api/model-info`
Returns model architecture details and configuration.

---

## ğŸ¨ Frontend

Built with **Next.js 14** (TypeScript), deployed on Vercel.

**Key UI Components:**

| Component | Description |
|---|---|
| `hero-section.tsx` | Landing hero with animated entry |
| `upload-section.tsx` | Drag-and-drop image upload |
| `processing-pipeline.tsx` | Sequential animated reveal of pipeline stages |
| `output-dashboard.tsx` | Final segmentation results and class overlay |
| `statistics-panel.tsx` | Real-time metrics, pie chart, class breakdown |
| `terrain-3d.tsx` | Three.js 3D architecture visualization + `segheads.mp4` |
| `risk-gauge.tsx` | Animated risk level gauge (LOW/MEDIUM/HIGH) |
| `model-transparency.tsx` | LIME-based feature attribution explainability panel |

The 3D pipeline animation (`public/segheads.mp4`, 305MB) shows 7 segmentation heads with parallel branch processing and particle data-flow â€” rendered with Python + Matplotlib + FFmpeg via `animation.py`.

---

## ğŸ—‚ï¸ Project Structure

```
ByteWorks-Desert_Perception_System/
â”‚
â”œâ”€â”€ api.py                          # FastAPI application entry point
â”œâ”€â”€ main.py                         # CLI entry point / local testing
â”œâ”€â”€ animation.py                    # 3D pipeline animation renderer
â”œâ”€â”€ train_segment.py                # Training script
â”œâ”€â”€ test_robustness.py              # Robustness evaluation pipeline
â”œâ”€â”€ download_model.py               # Model weight downloader
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ pyproject.toml                  # Project metadata (uv)
â”œâ”€â”€ render.yaml                     # Render.com deployment config
â”‚
â”œâ”€â”€ umixformer_pipeline/            # Core model code
â”‚   â”œâ”€â”€ model.py                    # U-MixFormer architecture
â”‚   â”œâ”€â”€ config.py                   # Model + training config
â”‚   â”œâ”€â”€ evaluate.py                 # Evaluation loop
â”‚   â”œâ”€â”€ metrics.py                  # mIoU, pixel accuracy, etc.
â”‚   â””â”€â”€ checkpoints/
â”‚       â””â”€â”€ umixformer_best.pth     # Best model weights
â”‚
â”œâ”€â”€ inference_engine/               # Optimized inference wrapper
â”‚   â”œâ”€â”€ model.py                    # Inference-only model wrapper
â”‚   â”œâ”€â”€ config.py                   # Inference config
â”‚   â””â”€â”€ utils.py                    # Pre/post-processing utilities
â”‚
â”œâ”€â”€ offroad_training_pipeline/      # Domain-specific training pipeline
â”œâ”€â”€ Offroad_Segmentation_Scripts/   # Dataset preprocessing scripts
â”‚
â”œâ”€â”€ Hardware Code/                  # Embedded firmware (UGV sensors)
â”œâ”€â”€ IR_UV_Scripts/                  # IR/UV camera processing scripts
â”œâ”€â”€ IR_Ultrasonic Models/           # Sensor fusion models
â”œâ”€â”€ Image Processing Algs/          # Classical CV preprocessing
â”œâ”€â”€ scripts/                        # Utility and helper scripts
â”‚
â”œâ”€â”€ frontend/                       # Next.js web application
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ layout.tsx
â”‚   â”‚   â”œâ”€â”€ page.tsx
â”‚   â”‚   â””â”€â”€ globals.css
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ hero-section.tsx
â”‚   â”‚   â”œâ”€â”€ upload-section.tsx
â”‚   â”‚   â”œâ”€â”€ processing-pipeline.tsx
â”‚   â”‚   â”œâ”€â”€ output-dashboard.tsx
â”‚   â”‚   â”œâ”€â”€ statistics-panel.tsx
â”‚   â”‚   â”œâ”€â”€ terrain-3d.tsx
â”‚   â”‚   â”œâ”€â”€ risk-gauge.tsx
â”‚   â”‚   â”œâ”€â”€ model-transparency.tsx
â”‚   â”‚   â””â”€â”€ ui/
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”œâ”€â”€ use-mobile.ts
â”‚   â”‚   â””â”€â”€ use-toast.ts
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â””â”€â”€ segheads.mp4            # 3D animation (305MB)
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ DEPLOYMENT_GUIDE.md
â”œâ”€â”€ QUICKSTART.md
â”œâ”€â”€ PROJECT_STRUCTURE.txt
â””â”€â”€ results.txt
```

---

## âš™ï¸ Setup & Installation

### Prerequisites

```
Python 3.11+
CUDA 12.1+ (recommended for GPU inference)
Node.js 18+
pnpm (or npm/yarn)
uv (Python package manager)
```

### Backend

```bash
# Clone the repo
git clone https://github.com/SPIT-Hackathon-2026/ByteWorks-Desert_Perception_System.git
cd ByteWorks-Desert_Perception_System

# Create and activate Python environment
python -m venv .venv
source .venv/bin/activate        # Windows: .venv\Scripts\activate

# Install dependencies
pip install uv
uv pip install -r requirements.txt

# Download model weights
uv run python download_model.py

# Start the API server
uv run uvicorn api:app --host 0.0.0.0 --port 8000 --reload
# API available at http://localhost:8000
# Swagger docs at http://localhost:8000/docs
```

### Frontend

```bash
cd frontend

# Install dependencies
pnpm install

# Start development server
pnpm dev
# Open http://localhost:3000
```

### Running Robustness Tests

```bash
# From project root
uv run python test_robustness.py
# Outputs saved to dataset/results_better/
```

---

## ğŸš€ Deployment

### Frontend (Vercel)

```bash
cd frontend
vercel --prod
```

### Backend (Render)

The `render.yaml` at the repo root configures auto-deployment. Simply push to `main`:

```bash
git add .
git commit -m "Deploy: <description>"
git push origin main
# Render detects the push and auto-redeploys
```

Verify the deployment:

```bash
curl https://semantic-segmentation-api.onrender.com/api/health
# {"status":"ok","model":"U-MixFormer","device":"cuda"}
```

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology | Role |
|---|---|---|
| Deep Learning | PyTorch 2.10 | Model training & inference |
| Architecture | U-MixFormer + ConvNeXt | Semantic segmentation backbone + decoder |
| API | FastAPI + Uvicorn | High-performance async REST API |
| Frontend | Next.js 14 + React | Web interface |
| 3D Rendering | Three.js + React Three Fiber | Architecture visualization |
| Animations | Framer Motion | UI transitions |
| Styling | Tailwind CSS v4 | Utility-first CSS |
| Export | Matplotlib + FFmpeg | 3D animation video generation |
| Hardware | Arduino / Embedded C | UGV sensor firmware |
| Sensor Processing | MATLAB | IR/UV image analysis |
| Frontend Host | Vercel | CDN + auto-scaling |
| Backend Host | Render | GPU cloud server |
| Package Manager | uv | Fast Python dependency management |

---

## ğŸ‘¥ Contributors

| Name | GitHub |
|---|---|
| Raj | [@CodeCraftsmanRaj](https://github.com/CodeCraftsmanRaj) |
| Shivani Bhat | [@shivanibhat24](https://github.com/shivanibhat24) |

---

## ğŸ“„ License

This project was developed for **SPIT Hackathon 2026** by Team ByteWorks. All rights reserved by the contributors.

---

*Last updated: February 22, 2026 Â· Status: âœ… Production Ready*
